# -*- coding: utf-8 -*-
"""Hybrid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B7SmR_8KSXBfFiKYmVjpaGWzkBhHqJT5
"""

import subprocess
import sys

# Install required packages for Colab
try:
    import google.colab
    print("Installing in Colab...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "sentence-transformers"])
except:
    pass

from __future__ import annotations
import gc
import logging
import math
import warnings
from collections import defaultdict, Counter
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize
from sklearn.decomposition import TruncatedSVD

warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s", force=True)

try:
    from sentence_transformers import SentenceTransformer
    _HAS_SENTENCE_TRANSFORMERS = True
except ImportError:
    _HAS_SENTENCE_TRANSFORMERS = False

from sklearn.feature_extraction.text import TfidfVectorizer

# Data paths
DATA_DIR = Path(".")
TRAIN_F = DATA_DIR / "train.csv"
TEST_F = DATA_DIR / "test.csv"
META_F = DATA_DIR / "item_meta.csv"
SUB_F = DATA_DIR / "sample_submission.csv"

# Check files exist
for f in [TRAIN_F, TEST_F, META_F, SUB_F]:
    if not f.exists():
        try:
            from google.colab import files
            print(f"Missing {f.name}. Please upload files:")
            uploaded = files.upload()
        except:
            raise FileNotFoundError(f"Missing {f}")

def enhance_meta_with_bought_together(train, meta, session_gap_hours=1, min_support=3, max_items=5):
    """
    Enhance metadata with bought_together patterns based on purchase behavior
    """
    logging.info("Enhancing metadata with bought-together patterns...")

    train_sorted = train.sort_values(['user_id', 'timestamp'])
    bought_together_patterns = defaultdict(lambda: defaultdict(float))

    # Session-based analysis
    session_gap_seconds = session_gap_hours * 3600
    user_sessions = defaultdict(list)

    for _, row in train_sorted.iterrows():
        session_key = (row["user_id"], row["timestamp"] // session_gap_seconds)
        user_sessions[session_key].append(row["item_id"])

    # Count co-occurrences with weights for session-based patterns
    session_pairs = 0
    for session_items in user_sessions.values():
        if len(session_items) >= 2:
            unique_items = list(dict.fromkeys(session_items))

            for i in range(len(unique_items)):
                for j in range(i+1, len(unique_items)):
                    item1, item2 = unique_items[i], unique_items[j]
                    bought_together_patterns[item1][item2] += 2.0
                    bought_together_patterns[item2][item1] += 2.0
                    session_pairs += 1

    # User co-occurrence patterns
    user_items = train.groupby('user_id')['item_id'].apply(lambda x: list(set(x))).to_dict()

    user_pairs = 0
    for items in user_items.values():
        if len(items) >= 2:
            for i in range(len(items)):
                for j in range(i+1, len(items)):
                    item1, item2 = items[i], items[j]
                    bought_together_patterns[item1][item2] += 0.5
                    bought_together_patterns[item2][item1] += 0.5
                    user_pairs += 1

    # Sequential patterns
    user_sequences = train_sorted.groupby('user_id')['item_id'].apply(list).to_dict()

    sequential_pairs = 0
    for user_id, sequence in user_sequences.items():
        for i in range(len(sequence)):
            for j in range(i+1, min(i+4, len(sequence))):
                if sequence[i] != sequence[j]:
                    weight = 1.5 / (j - i)
                    bought_together_patterns[sequence[i]][sequence[j]] += weight
                    sequential_pairs += 1

    logging.info(f"Found {session_pairs:,} session pairs, {user_pairs:,} user pairs, {sequential_pairs:,} sequential pairs")

    # Generate bought_together strings
    bought_together_data = {}
    items_with_patterns = 0

    for item, related_items in bought_together_patterns.items():
        strong_patterns = {k: v for k, v in related_items.items() if v >= min_support}

        if strong_patterns:
            sorted_patterns = sorted(strong_patterns.items(), key=lambda x: x[1], reverse=True)
            top_items = [str(item_id) for item_id, _ in sorted_patterns[:max_items]]
            bought_together_data[item] = ",".join(top_items)
            items_with_patterns += 1

    # Update meta dataframe
    meta = meta.copy()
    if 'bought_together' not in meta.columns:
        meta['bought_together'] = ""

    meta['bought_together'] = meta['item_id'].map(bought_together_data).fillna("")

    filled_count = (meta['bought_together'] != "").sum()
    logging.info(f"Filled bought_together for {filled_count:,}/{len(meta):,} items ({filled_count/len(meta)*100:.1f}%)")

    return meta

def load_data_with_enhancements():
    """Load data and enhance with bought_together patterns"""
    logging.info("Loading data with enhancements...")
    train = pd.read_csv(TRAIN_F)
    test = pd.read_csv(TEST_F)
    meta = pd.read_csv(META_F)
    sub = pd.read_csv(SUB_F)

    # Ensure proper types
    train["user_id"] = train["user_id"].astype(int)
    train["item_id"] = train["item_id"].astype(int)
    train["timestamp"] = train["timestamp"].astype(int)

    # Remove duplicates - keep LAST interaction
    train = train.sort_values("timestamp").drop_duplicates(["user_id", "item_id"], keep="last")
    test = test.sort_values("timestamp").drop_duplicates(["user_id", "item_id"], keep="last")

    # Enhance meta with bought_together patterns
    meta = enhance_meta_with_bought_together(train, meta)

    # Save enhanced metadata
    meta.to_csv("item_meta_enhanced.csv", index=False)
    logging.info("Enhanced metadata saved as 'item_meta_enhanced.csv'")

    logging.info(f"Train: {len(train):,} | Test: {len(test):,}")
    logging.info(f"Users: {train['user_id'].nunique():,} | Items: {train['item_id'].nunique():,}")
    return train, test, meta, sub

def wilson_score(positive, total, z=1.96):
    """Wilson score for better popularity estimation"""
    if total == 0:
        return 0
    phat = positive / total
    denominator = 1 + z**2 / total
    centre_adjusted = (phat + z**2 / (2 * total)) / denominator
    interval = z * math.sqrt((phat * (1 - phat) + z**2 / (4 * total)) / total) / denominator
    return centre_adjusted - interval

def quality_score(row):
    """Enhanced quality score with Wilson adjustment"""
    rating = row.get("average_rating", 0)
    if pd.isna(rating) or rating == 0:
        return 0.0
    count = row.get("rating_number", 0)
    if pd.isna(count) or count == 0:
        return rating / 5.0

    ws = wilson_score(rating * count / 5, count)
    volume_boost = math.log1p(count) / 10
    return ws + volume_boost

def build_popularity(train, meta):
    """Build time-aware popularity rankings with burst detection"""
    max_ts = train["timestamp"].max()
    min_ts = train["timestamp"].min()
    time_span = max_ts - min_ts

    train_copy = train.copy()
    train_copy["time_weight"] = np.exp(-2 * (max_ts - train["timestamp"]) / time_span)

    # Detect trending items
    recent_window = train["timestamp"].quantile(0.8)
    old_window = train["timestamp"].quantile(0.4)

    recent_counts = train[train["timestamp"] >= recent_window]["item_id"].value_counts()
    old_counts = train[train["timestamp"] < old_window]["item_id"].value_counts()

    trend_scores = {}
    for item in recent_counts.index:
        recent = recent_counts.get(item, 0)
        old = old_counts.get(item, 0)
        if old > 0:
            trend_scores[item] = recent / (old + 1)
        else:
            trend_scores[item] = recent * 2

    item_scores = train_copy.groupby("item_id").agg({
        "time_weight": "sum",
        "user_id": "nunique"
    })

    q_scores = {}
    for _, row in meta.iterrows():
        q_scores[row["item_id"]] = quality_score(row)

    scores = {}
    for item in item_scores.index:
        popularity = item_scores.loc[item, "time_weight"]
        unique_users = item_scores.loc[item, "user_id"]
        diversity = math.log1p(unique_users) / 10
        quality = q_scores.get(item, 0)
        trend = trend_scores.get(item, 1.0)

        scores[item] = 0.4 * popularity + 0.3 * quality + 0.15 * diversity + 0.15 * math.log1p(trend)

    return sorted(scores.keys(), key=lambda x: scores[x], reverse=True)

def build_category_popularity(train, meta):
    """Enhanced category popularity with quality weighting"""
    cat_col = None
    if "main_category" in meta.columns:
        cat_col = "main_category"
    elif "category" in meta.columns:
        cat_col = "category"
    else:
        logging.warning("No category column found")
        return {}

    q_scores = {}
    for _, row in meta.iterrows():
        q_scores[row["item_id"]] = quality_score(row)

    item_cats = dict(zip(meta["item_id"], meta[cat_col].fillna("unknown")))
    cat_items = defaultdict(list)

    item_pop = train["item_id"].value_counts()

    for item, count in item_pop.items():
        cat = item_cats.get(item, "unknown")
        quality = q_scores.get(item, 0)
        score = count * (1 + quality)
        cat_items[cat].append((item, score))

    for cat in cat_items:
        cat_items[cat].sort(key=lambda x: x[1], reverse=True)
        cat_items[cat] = [item for item, _ in cat_items[cat]]

    return dict(cat_items)

class OptimizedSequentialRec:
    """Optimized sequential recommender with tier-specific strategies and bought_together enhancement"""

    def __init__(self, train, meta):
        self.train = train
        self.meta = meta

        logging.info("Building optimized recommender...")

        # Category mapping
        if "main_category" in meta.columns:
            self.item_cat = dict(zip(meta["item_id"], meta["main_category"].fillna("unknown")))
        elif "category" in meta.columns:
            self.item_cat = dict(zip(meta["item_id"], meta["category"].fillna("unknown")))
        else:
            self.item_cat = {}

        # User histories
        self.user_hist = train.sort_values("timestamp").groupby("user_id")["item_id"].apply(list).to_dict()

        # User tier classification
        self.user_tiers = self._classify_users()

        # Core components
        self.global_pop = build_popularity(train, meta)
        self.cat_pop = build_category_popularity(train, meta)

        # Build all patterns
        self._build_transitions()
        self._build_repeat_patterns()
        self._build_similarity()
        self._build_item_item_cf()
        self._build_price_patterns()
        self._build_bundle_patterns()
        self._build_svd_embeddings()
        self._build_metadata_bought_together()

        # User profiles
        self._build_user_profiles()

        # Caches for speed
        self._sim_cache = {}
        self._cf_cache = {}
        self._bt_cache = {}

        logging.info("Optimized recommender ready")

    def _classify_users(self):
        """Classify users into tiers based on interaction count"""
        user_tiers = {}
        for user_id, items in self.user_hist.items():
            interaction_count = len(items)
            if interaction_count == 1:
                user_tiers[user_id] = "cold"
            elif 2 <= interaction_count <= 4:
                user_tiers[user_id] = "warm"
            else:
                user_tiers[user_id] = "hot"
        return user_tiers

    def _build_metadata_bought_together(self):
        """Build bought together patterns from enhanced metadata"""
        self.metadata_bought_together = defaultdict(list)

        if 'bought_together' in self.meta.columns:
            logging.info("Using enhanced metadata bought_together patterns...")

            pattern_count = 0
            for _, row in self.meta.iterrows():
                item_id = row['item_id']
                bought_together = str(row.get('bought_together', '')).strip()

                if bought_together and bought_together != 'nan' and bought_together != '':
                    try:
                        related_items = [int(x.strip()) for x in bought_together.split(',') if x.strip()]
                        self.metadata_bought_together[item_id] = related_items
                        pattern_count += len(related_items)
                    except ValueError:
                        continue

            logging.info(f"Loaded bought-together patterns for {len(self.metadata_bought_together)} items")
        else:
            logging.info("No bought_together column found in metadata")

    def get_bought_together_items(self, item_id, k=10):
        """Get items frequently bought together from enhanced metadata"""
        if item_id in self._bt_cache:
            return self._bt_cache[item_id][:k]

        if item_id not in self.metadata_bought_together:
            return []

        bought_together = self.metadata_bought_together[item_id][:k]
        self._bt_cache[item_id] = bought_together
        return bought_together

    def _build_transitions(self):
        """Build multi-order transition patterns with regularization"""
        self.transitions = defaultdict(lambda: defaultdict(float))
        self.transitions_2 = defaultdict(lambda: defaultdict(float))
        self.cooccur = defaultdict(lambda: defaultdict(float))
        self.session_patterns = defaultdict(lambda: defaultdict(float))

        sequences = self.train.sort_values("timestamp").groupby("user_id")["item_id"].apply(list)
        timestamps = self.train.sort_values("timestamp").groupby("user_id")["timestamp"].apply(list)

        SESSION_GAP = 3600  # 1 hour session gap

        for user_id, seq in sequences.items():
            ts_list = timestamps.get(user_id, [])

            # Direct transitions
            for i in range(len(seq) - 1):
                self.transitions[seq[i]][seq[i+1]] += 1.0

                # Session-based boost
                if i < len(ts_list) - 1 and ts_list[i+1] - ts_list[i] < SESSION_GAP:
                    self.session_patterns[seq[i]][seq[i+1]] += 1.0

            # 2-hop transitions
            for i in range(len(seq) - 2):
                self.transitions_2[seq[i]][seq[i+2]] += 0.3

            # Co-occurrences with distance decay
            max_window = min(8, len(seq))
            for i in range(len(seq)):
                for j in range(i+1, min(i+max_window, len(seq))):
                    dist = j - i
                    weight = 1.0 / math.sqrt(dist)
                    self.cooccur[seq[i]][seq[j]] += weight
                    self.cooccur[seq[j]][seq[i]] += weight

        # Normalize with Laplace smoothing
        for patterns in [self.transitions, self.transitions_2, self.cooccur, self.session_patterns]:
            for item in list(patterns.keys()):
                total = sum(patterns[item].values())
                if total > 0:
                    smoothed_total = total + len(patterns[item]) * 0.1
                    patterns[item] = {
                        k: (v + 0.1) / smoothed_total for k, v in patterns[item].items()
                    }

    def _build_repeat_patterns(self):
        """Build user repeat purchase patterns"""
        self.repeat_items = defaultdict(set)
        self.item_repeat_rate = defaultdict(float)

        user_item_counts = self.train.groupby(["user_id", "item_id"]).size()

        for (user_id, item_id), count in user_item_counts.items():
            if count > 1:
                self.repeat_items[user_id].add(item_id)

        item_users = self.train.groupby("item_id")["user_id"].apply(set)
        repeat_users = defaultdict(int)

        for user_id, items in self.repeat_items.items():
            for item in items:
                repeat_users[item] += 1

        for item, users in item_users.items():
            if len(users) > 0:
                self.item_repeat_rate[item] = repeat_users.get(item, 0) / len(users)

    def _build_user_profiles(self):
        """Build detailed user preference profiles with regularization"""
        self.user_cat_prefs = defaultdict(lambda: defaultdict(float))
        self.user_item_times = defaultdict(list)
        self.user_active_period = {}

        for _, row in self.train.iterrows():
            user = row["user_id"]
            item = row["item_id"]
            ts = row["timestamp"]

            cat = self.item_cat.get(item, "unknown")
            recency = 1.0
            self.user_cat_prefs[user][cat] += recency

            self.user_item_times[user].append((ts, item))

        # Normalize preferences with smoothing
        for user, prefs in self.user_cat_prefs.items():
            total = sum(prefs.values())
            if total > 0:
                smoothed_total = total + len(prefs) * 0.1
                self.user_cat_prefs[user] = {
                    cat: (score + 0.1) / smoothed_total for cat, score in prefs.items()
                }

        # Calculate user activity periods
        for user, times in self.user_item_times.items():
            if times:
                sorted_times = sorted([t[0] for t in times])
                self.user_active_period[user] = (sorted_times[0], sorted_times[-1])

    def _build_similarity(self):
        """Build fast content similarity"""
        train_items = set(self.train["item_id"].unique())
        meta_filtered = self.meta[self.meta["item_id"].isin(train_items)].copy()
        meta_filtered["details"] = meta_filtered["details"].fillna("")

        if "title" not in meta_filtered.columns:
            logging.warning("No title column - skipping content similarity")
            self.has_similarity = False
            return

        meta_filtered["title"] = meta_filtered["title"].fillna("unknown")

        # Add category to text for better similarity
        if hasattr(self, 'item_cat') and self.item_cat:
            meta_filtered["text"] = meta_filtered.apply(
                lambda x: f"{x['title']} {x['details']} {self.item_cat.get(x['item_id'], '')}",
                axis=1
            )
        else:
            meta_filtered["text"] = meta_filtered.apply(
                lambda x: f"{x['title']} {x['details']}", axis=1
            )

        if _HAS_SENTENCE_TRANSFORMERS:
            logging.info("Building embeddings with enhanced text...")
            model = SentenceTransformer('all-MiniLM-L6-v2')
            texts = meta_filtered["text"].astype(str).tolist()

            embeddings = model.encode(texts, batch_size=64, show_progress_bar=True)
            embeddings = normalize(embeddings, norm='l2')

            self.item_idx = dict(zip(meta_filtered["item_id"], range(len(meta_filtered))))
            self.idx_item = {v: k for k, v in self.item_idx.items()}
            self.embeddings = embeddings
        else:
            logging.info("Using TF-IDF with enhanced text...")
            tfidf = TfidfVectorizer(
                max_features=30000,
                stop_words='english',
                ngram_range=(1, 2),
                min_df=2
            )
            texts = meta_filtered["text"].astype(str).tolist()

            tfidf_matrix = tfidf.fit_transform(texts)

            self.item_idx = dict(zip(meta_filtered["item_id"], range(len(meta_filtered))))
            self.idx_item = {v: k for k, v in self.item_idx.items()}
            self.embeddings = tfidf_matrix

        self.has_similarity = True

    def _build_item_item_cf(self):
        """Build simple item-item collaborative filtering"""
        self.item_cf = defaultdict(lambda: defaultdict(float))

        user_items = self.train.groupby("user_id")["item_id"].apply(set).to_dict()

        item_users = defaultdict(set)
        for user, items in user_items.items():
            for item in items:
                item_users[item].add(user)

        # Calculate Jaccard similarities for popular items only
        popular_items = [item for item, users in item_users.items() if len(users) >= 5]

        logging.info(f"Building CF for {len(popular_items)} popular items...")

        for i, item1 in enumerate(popular_items):
            if i % 1000 == 0:
                logging.info(f"CF progress: {i}/{len(popular_items)}")

            users1 = item_users[item1]

            candidates = set()
            for user in users1:
                candidates.update(user_items[user])
            candidates.remove(item1)

            for item2 in candidates:
                users2 = item_users[item2]

                intersection = len(users1 & users2)
                if intersection >= 2:
                    union = len(users1 | users2)
                    jaccard = intersection / union
                    self.item_cf[item1][item2] = jaccard

    def _build_price_patterns(self):
        """Build price preference patterns"""
        self.item_prices = {}
        self.user_price_ranges = {}

        if "price" in self.meta.columns:
            self.item_prices = dict(zip(self.meta["item_id"], self.meta["price"].fillna(0)))

            for user, items in self.user_hist.items():
                prices = [self.item_prices.get(item, 0) for item in items if self.item_prices.get(item, 0) > 0]
                if prices:
                    self.user_price_ranges[user] = {
                        "min": np.percentile(prices, 10),
                        "max": np.percentile(prices, 90),
                        "median": np.median(prices)
                    }
        else:
            logging.info("No price data available")

    def _build_bundle_patterns(self):
        """Build frequently bought together patterns"""
        self.bundle_patterns = defaultdict(lambda: defaultdict(float))

        user_sessions = defaultdict(list)

        for _, row in self.train.iterrows():
            session_key = (row["user_id"], row["timestamp"] // 3600)
            user_sessions[session_key].append(row["item_id"])

        for session_items in user_sessions.values():
            if len(session_items) >= 2:
                for i in range(len(session_items)):
                    for j in range(i+1, len(session_items)):
                        self.bundle_patterns[session_items[i]][session_items[j]] += 1.0
                        self.bundle_patterns[session_items[j]][session_items[i]] += 1.0

        # Normalize
        for item in self.bundle_patterns:
            total = sum(self.bundle_patterns[item].values())
            if total > 0:
                self.bundle_patterns[item] = {
                    k: v/total for k, v in self.bundle_patterns[item].items()
                }

        logging.info(f"Found bundle patterns for {len(self.bundle_patterns)} items")

    def _build_svd_embeddings(self):
        """Build user/item embeddings using SVD"""
        self.has_svd = False

        try:
            user_ids = sorted(self.train["user_id"].unique())
            item_ids = sorted(self.train["item_id"].unique())

            if len(user_ids) > 100000 or len(item_ids) > 50000:
                logging.info("Too many users/items for SVD - skipping")
                return

            user_idx = {u: i for i, u in enumerate(user_ids)}
            item_idx = {i: j for j, i in enumerate(item_ids)}

            row_ind = [user_idx[u] for u in self.train["user_id"]]
            col_ind = [item_idx[i] for i in self.train["item_id"]]
            data = np.ones(len(row_ind))

            interaction_matrix = csr_matrix(
                (data, (row_ind, col_ind)),
                shape=(len(user_ids), len(item_ids))
            )

            logging.info("Building SVD embeddings...")
            svd = TruncatedSVD(n_components=50, random_state=42)
            user_embeddings = svd.fit_transform(interaction_matrix)
            item_embeddings = svd.components_.T

            user_embeddings = normalize(user_embeddings, norm='l2')
            item_embeddings = normalize(item_embeddings, norm='l2')

            self.user_idx_svd = user_idx
            self.item_idx_svd = item_idx
            self.user_embeddings = user_embeddings
            self.item_embeddings_svd = item_embeddings
            self.idx_to_item = {v: k for k, v in item_idx.items()}

            self.has_svd = True
            logging.info(f"SVD embeddings: {len(user_ids)} users × {len(item_ids)} items")

        except Exception as e:
            logging.warning(f"SVD failed: {e}")
            self.has_svd = False

    def get_similar_items(self, item_id, k=20):
        """Get similar items using content similarity"""
        if item_id in self._sim_cache:
            return self._sim_cache[item_id][:k]

        if not self.has_similarity or item_id not in self.item_idx:
            return []

        idx = self.item_idx[item_id]
        sims = cosine_similarity(self.embeddings[idx:idx+1], self.embeddings).flatten()

        k_val = min(k+1, len(sims)-1)
        top_idx = np.argpartition(sims, -k_val)[-k_val:]
        top_idx = top_idx[sims[top_idx].argsort()][::-1]

        similar = []
        for i in top_idx:
            if i != idx:
                similar.append(self.idx_item[i])

        self._sim_cache[item_id] = similar
        return similar[:k]

    def get_cf_items(self, item_id, k=10):
        """Get collaborative filtering recommendations"""
        if item_id in self._cf_cache:
            return self._cf_cache[item_id][:k]

        if item_id not in self.item_cf:
            return []

        sorted_items = sorted(
            self.item_cf[item_id].items(),
            key=lambda x: x[1],
            reverse=True
        )

        cf_recs = [item for item, _ in sorted_items[:k]]
        self._cf_cache[item_id] = cf_recs
        return cf_recs

    def get_svd_recommendations(self, user_id, k=20):
        """Get recommendations using SVD embeddings"""
        if not self.has_svd or user_id not in self.user_idx_svd:
            return []

        user_idx = self.user_idx_svd[user_id]
        user_vec = self.user_embeddings[user_idx:user_idx+1]

        similarities = cosine_similarity(user_vec, self.item_embeddings_svd).flatten()

        top_indices = np.argpartition(similarities, -k)[-k:]
        top_indices = top_indices[similarities[top_indices].argsort()][::-1]

        recommendations = []
        for idx in top_indices:
            item_id = self.idx_to_item[idx]
            recommendations.append(item_id)

        return recommendations[:k]

    def recommend(self, user_id, k=10):
        """Generate tier-specific recommendations with bought_together enhancement"""
        seen = set(self.user_hist.get(user_id, []))
        scores = defaultdict(float)

        user_items = self.user_hist.get(user_id, [])
        user_tier = self.user_tiers.get(user_id, "cold")

        if not user_items:
            # Cold start - return popular items
            popular_recs = []
            used_cats = set()
            for item in self.global_pop:
                if item not in seen:
                    cat = self.item_cat.get(item, "unknown")
                    if len(used_cats) < 3 or cat in used_cats:
                        popular_recs.append(item)
                        used_cats.add(cat)
                        if len(popular_recs) >= k:
                            break
            return popular_recs[:k]

        # Tier-specific strategy
        if user_tier == "cold":
            weights = {
                'content': 2.0,
                'popularity': 1.67,
                'category': 1.2,
                'sequential': 0.8,
                'cf': 0.5,
                'bought_together': 1.8
            }
        elif user_tier == "warm":
            weights = {
                'sequential': 2.5,
                'content': 1.8,
                'cf': 1.3,
                'category': 1.0,
                'popularity': 0.8,
                'bought_together': 2.2
            }
        else:  # hot users
            weights = {
                'sequential': 2.5,
                'cf': 1.5,
                'content': 1.8,
                'popularity': 1.3,
                'category': 0.8,
                'bought_together': 1.9
            }

        recent_window = min(8 if user_tier == "hot" else 6, len(user_items))
        recent = user_items[-recent_window:]

        # 1. Sequential predictions
        for i, item in enumerate(reversed(recent)):
            pos_weight = 0.85 ** i

            if item in self.transitions:
                limit = 30 if user_tier == "hot" else 50
                for next_item, prob in list(self.transitions[item].items())[:limit]:
                    if next_item not in seen:
                        scores[next_item] += pos_weight * prob * weights['sequential']

            if item in self.session_patterns and i < (2 if user_tier == "hot" else 3):
                limit = 15 if user_tier == "hot" else 20
                for next_item, prob in list(self.session_patterns[item].items())[:limit]:
                    if next_item not in seen:
                        scores[next_item] += pos_weight * prob * weights['sequential'] * 0.8

            if item in self.transitions_2 and user_tier != "hot":
                for next_item, prob in list(self.transitions_2[item].items())[:20]:
                    if next_item not in seen:
                        scores[next_item] += pos_weight * prob * weights['sequential'] * 0.5

            if item in self.cooccur:
                limit = 25 if user_tier == "hot" else 40
                for next_item, prob in list(self.cooccur[item].items())[:limit]:
                    if next_item not in seen:
                        scores[next_item] += pos_weight * prob * weights['sequential'] * 0.6

        # 2. Content-based recommendations
        content_items = min(5 if user_tier == "hot" else 7, len(recent))
        for i, item in enumerate(reversed(recent[:content_items])):
            weight = (0.8 ** i) * weights['content']
            similar_items = self.get_similar_items(item, k=12)

            for rank, sim_item in enumerate(similar_items):
                if sim_item not in seen:
                    scores[sim_item] += weight * (1.0 / (rank + 1))

        # 3. Collaborative filtering
        cf_items = min(3 if user_tier == "hot" else 5, len(recent))
        for i, item in enumerate(reversed(recent[:cf_items])):
            weight = (0.7 ** i) * weights['cf']
            cf_recs = self.get_cf_items(item, k=8)

            for rank, cf_item in enumerate(cf_recs):
                if cf_item not in seen:
                    scores[cf_item] += weight * (1.0 / (rank + 1))

        # 4. SVD-based recommendations
        if self.has_svd:
            svd_k = 20 if user_tier != "hot" else 15
            svd_recs = self.get_svd_recommendations(user_id, k=svd_k)
            for rank, item in enumerate(svd_recs):
                if item not in seen:
                    scores[item] += (1.0 / (rank + 1)) * 0.8

        # 5. Repeat purchase boost
        if user_id in self.repeat_items:
            repeat_weight = 0.3 if user_tier == "hot" else 0.5
            for item in self.repeat_items[user_id]:
                if item not in seen:
                    scores[item] += repeat_weight

        for item, score in list(scores.items()):
            repeat_rate = self.item_repeat_rate.get(item, 0)
            if repeat_rate > 0.1:
                boost = 1 + (repeat_rate * 0.5 if user_tier == "hot" else repeat_rate)
                scores[item] *= boost

        # 6. Bundle recommendations
        bundle_items = min(2 if user_tier == "hot" else 3, len(recent))
        for item in recent[:bundle_items]:
            if item in self.bundle_patterns:
                bundle_limit = 15 if user_tier == "hot" else 20
                for bundle_item, bundle_score in list(self.bundle_patterns[item].items())[:bundle_limit]:
                    if bundle_item not in seen:
                        scores[bundle_item] += bundle_score * 1.0

        # 7. Price-aware recommendations
        if user_id in self.user_price_ranges and self.item_prices:
            user_range = self.user_price_ranges[user_id]
            for item, current_score in list(scores.items()):
                price = self.item_prices.get(item, 0)
                if price > 0:
                    if user_range["min"] <= price <= user_range["max"]:
                        scores[item] *= 1.15
                    elif price < user_range["min"] * 0.5 or price > user_range["max"] * 2:
                        scores[item] *= 0.85

        # 8. Category preferences
        user_cat_pref = self.user_cat_prefs.get(user_id, {})
        if user_cat_pref:
            top_cats = sorted(user_cat_pref.items(), key=lambda x: x[1], reverse=True)[:3]

            for cat, pref_score in top_cats:
                if cat in self.cat_pop:
                    cat_limit = 30 if user_tier == "hot" else 50
                    for rank, item in enumerate(self.cat_pop[cat][:cat_limit]):
                        if item not in seen:
                            scores[item] += pref_score * (1.0 / (rank + 5)) * weights['category']

        # 9. Global popularity
        pop_limit = 150 if user_tier == "hot" else 200
        for rank, item in enumerate(self.global_pop[:pop_limit]):
            if item not in seen:
                scores[item] += (1.0 / (rank + 50)) * weights['popularity']

        # 10. Bought-together recommendations
        bt_items = min(5 if user_tier == "hot" else 7, len(recent))
        for i, item in enumerate(reversed(recent[:bt_items])):
            pos_weight = (0.85 ** i) * weights['bought_together']
            bt_recs = self.get_bought_together_items(item, k=12)

            for rank, bt_item in enumerate(bt_recs):
                if bt_item not in seen:
                    rank_score = 1.0 / (rank + 1)
                    position_bonus = 1.2 if i == 0 else 1.0

                    score_boost = pos_weight * rank_score * position_bonus
                    scores[bt_item] += score_boost

        # 11. Combo bonus for items in both content similarity AND bought_together
        combo_bonus_items = set()
        for i, item in enumerate(reversed(recent[:3])):
            similar_items = set(self.get_similar_items(item, k=15))
            bt_items_set = set(self.get_bought_together_items(item, k=15))

            combo_items = similar_items & bt_items_set
            combo_bonus_items.update(combo_items)

        for item in combo_bonus_items:
            if item not in seen and item in scores:
                scores[item] *= 1.25

        # Score normalization and diversity
        if scores:
            max_score = max(scores.values())
            if max_score > 0:
                scores = {item: score/max_score for item, score in scores.items()}

            candidates = sorted(scores.items(), key=lambda x: x[1], reverse=True)

            final_recs = []
            used_cats = defaultdict(int)
            max_per_category = 4 if user_tier == "cold" else 5

            for item, score in candidates:
                cat = self.item_cat.get(item, "unknown")

                if used_cats[cat] >= max_per_category:
                    score *= 0.6

                final_recs.append((item, score))
                used_cats[cat] += 1

                if len(final_recs) >= k * 2:
                    break

            final_recs.sort(key=lambda x: x[1], reverse=True)
            recs = [item for item, _ in final_recs[:k]]
        else:
            recs = []

        # Fill with popular items if needed
        while len(recs) < k:
            for item in self.global_pop:
                if item not in seen and item not in recs:
                    recs.append(item)
                    if len(recs) == k:
                        break

        return recs

    def get_enhanced_recommendations(self, user_id, k=10):
        """Alias for compatibility"""
        return self.recommend(user_id, k)

def validate_optimized_system(hybrid_system, train, test):
    """
    Validation for the optimized recommendation system with 3-tier breakdown
    """
    logging.info("Validating system...")

    train_users = set(train["user_id"].unique())
    test_users = set(test["user_id"].unique())
    common_users = list(train_users & test_users)

    user_histories = train.sort_values("timestamp").groupby("user_id")["item_id"].apply(list).to_dict()
    user_interaction_counts = {user: len(items) for user, items in user_histories.items()}

    # 3-tier categorization
    one_interaction_users = [u for u in common_users if user_interaction_counts.get(u, 0) == 1]
    warm_interaction_users = [u for u in common_users if 2 <= user_interaction_counts.get(u, 0) <= 4]
    hot_interaction_users = [u for u in common_users if user_interaction_counts.get(u, 0) >= 5]

    test_items = test.groupby("user_id")["item_id"].apply(set).to_dict()

    one_sample = np.random.choice(one_interaction_users, min(1200, len(one_interaction_users)), replace=False) if one_interaction_users else []
    warm_sample = np.random.choice(warm_interaction_users, min(600, len(warm_interaction_users)), replace=False) if warm_interaction_users else []
    hot_sample = np.random.choice(hot_interaction_users, min(400, len(hot_interaction_users)), replace=False) if hot_interaction_users else []

    results = {
        'one_interaction': {'recalls': [], 'users': 0, 'hit_users': 0},
        'warm_interactions': {'recalls': [], 'users': 0, 'hit_users': 0},
        'hot_interactions': {'recalls': [], 'users': 0, 'hit_users': 0},
        'overall': {'recalls': [], 'users': 0, 'hit_users': 0, 'coverage': set()}
    }

    # Validate each tier
    for tier_name, user_sample in [
        ('one_interaction', one_sample),
        ('warm_interactions', warm_sample),
        ('hot_interactions', hot_sample)
    ]:
        logging.info(f"Validating {len(user_sample)} users in {tier_name} tier...")
        for user in user_sample:
            if user in test_items:
                predictions = hybrid_system.recommend(user, k=10)
                ground_truth = test_items[user]
                results['overall']['coverage'].update(predictions)

                if ground_truth:
                    hits = len(set(predictions) & ground_truth)
                    recall = hits / len(ground_truth)
                    results[tier_name]['recalls'].append(recall)
                    results[tier_name]['users'] += 1
                    if hits > 0:
                        results[tier_name]['hit_users'] += 1

    # Overall results
    all_recalls = (results['one_interaction']['recalls'] +
                  results['warm_interactions']['recalls'] +
                  results['hot_interactions']['recalls'])
    results['overall']['recalls'] = all_recalls
    results['overall']['users'] = (results['one_interaction']['users'] +
                                  results['warm_interactions']['users'] +
                                  results['hot_interactions']['users'])
    results['overall']['hit_users'] = (results['one_interaction']['hit_users'] +
                                      results['warm_interactions']['hit_users'] +
                                      results['hot_interactions']['hit_users'])

    # Print results
    logging.info("Validation Results:")
    logging.info("="*50)

    if results['one_interaction']['recalls']:
        one_recall = np.mean(results['one_interaction']['recalls'])
        one_hit_rate = results['one_interaction']['hit_users'] / results['one_interaction']['users']
        logging.info(f"1-Interaction Users:")
        logging.info(f"  Recall@10: {one_recall:.6f}")
        logging.info(f"  Hit Rate: {one_hit_rate:.1%}")

    if results['warm_interactions']['recalls']:
        warm_recall = np.mean(results['warm_interactions']['recalls'])
        warm_hit_rate = results['warm_interactions']['hit_users'] / results['warm_interactions']['users']
        logging.info(f"2-4 Interaction Users:")
        logging.info(f"  Recall@10: {warm_recall:.6f}")
        logging.info(f"  Hit Rate: {warm_hit_rate:.1%}")

    if results['hot_interactions']['recalls']:
        hot_recall = np.mean(results['hot_interactions']['recalls'])
        hot_hit_rate = results['hot_interactions']['hit_users'] / results['hot_interactions']['users']
        logging.info(f"5+ Interaction Users:")
        logging.info(f"  Recall@10: {hot_recall:.6f}")
        logging.info(f"  Hit Rate: {hot_hit_rate:.1%}")

    if results['overall']['recalls']:
        overall_recall = np.mean(results['overall']['recalls'])
        overall_hit_rate = results['overall']['hit_users'] / results['overall']['users']
        coverage_pct = len(results['overall']['coverage']) / train['item_id'].nunique() * 100
        logging.info(f"Overall Performance:")
        logging.info(f"  Recall@10: {overall_recall:.6f}")
        logging.info(f"  Hit Rate: {overall_hit_rate:.1%}")
        logging.info(f"  Catalog Coverage: {coverage_pct:.1f}%")

    return results

def optimized_make_submission(rec, sub_template, filename="enhanced_hybrid_submission.csv"):
    """
    Generate submission with progress tracking
    """
    logging.info("Generating submission...")

    results = []
    total_users = len(sub_template)

    for i, (_, row) in enumerate(sub_template.iterrows()):
        if i % 100 == 0:
            logging.info(f"Progress: {i}/{total_users} ({i/total_users*100:.1f}%)")

        user_id = int(row["user_id"])
        recs = rec.recommend(user_id, k=10)

        results.append({
            "ID": int(row["ID"]),
            "user_id": user_id,
            "item_id": ",".join(map(str, recs))
        })

    submission_df = pd.DataFrame(results)
    submission_df.to_csv(filename, index=False)

    logging.info(f"Submission saved as {filename}")

    # Try to download in Colab
    try:
        from google.colab import files
        files.download(filename)
        logging.info("File downloaded in Colab")
    except:
        pass

    return submission_df

def main():
    """
    Main function for the hybrid recommendation system
    """
    logging.info("Starting Hybrid Recommendation System")
    start_time = time.time()

    # Load data
    train, test, meta, sub = load_data_with_enhancements()

    # Dataset analysis
    logging.info("Dataset Analysis:")
    train_users = train['user_id'].nunique()
    train_items = train['item_id'].nunique()
    test_users = test['user_id'].nunique()
    test_items = test['item_id'].nunique()

    logging.info(f"Train: {len(train):,} interactions")
    logging.info(f"Test: {len(test):,} interactions")
    logging.info(f"Train Users: {train_users:,}")
    logging.info(f"Train Items: {train_items:,}")

    # User interaction distribution
    user_interaction_counts = train['user_id'].value_counts()
    cold_users_count = (user_interaction_counts == 1).sum()
    warm_users_count = ((user_interaction_counts >= 2) & (user_interaction_counts <= 4)).sum()
    hot_users_count = (user_interaction_counts >= 5).sum()

    logging.info(f"Cold users (1 interaction): {cold_users_count:,} ({cold_users_count/train_users*100:.1f}%)")
    logging.info(f"Warm users (2-4 interactions): {warm_users_count:,} ({warm_users_count/train_users*100:.1f}%)")
    logging.info(f"Hot users (5+ interactions): {hot_users_count:,} ({hot_users_count/train_users*100:.1f}%)")

    # Build recommender system
    rec = OptimizedSequentialRec(train, meta)

    # Validation
    validation_results = validate_optimized_system(rec, train, test)

    # Generate submission
    submission_df = optimized_make_submission(rec, sub, "enhanced_hybrid_submission.csv")

    # Final summary
    if validation_results and validation_results['overall']['recalls']:
        overall_recall = np.mean(validation_results['overall']['recalls'])
        overall_hit_rate = validation_results['overall']['hit_users'] / validation_results['overall']['users']

        logging.info("Performance Summary:")
        logging.info(f"Overall Recall@10: {overall_recall:.6f}")
        logging.info(f"Overall Hit Rate: {overall_hit_rate:.1%}")

    gc.collect()
    elapsed = time.time() - start_time
    logging.info(f"Total execution time: {elapsed:.1f} seconds")
    logging.info("Hybrid Recommendation System completed")

    return rec, validation_results, submission_df

if __name__ == "__main__":
    recommender, validation_results, submission = main()
    print("Hybrid System Completed")

